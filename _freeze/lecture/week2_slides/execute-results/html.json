{
  "hash": "c684c344a8507d4bfc6798a12af016d1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Algorithmic Decision Making & Census Data\"\nsubtitle: \"Week 2: CPLN 5920-MUSA 5080\"\nauthor: \"Allison Lassiter\"\ndate: \"January 27, 2026\"\nformat: \n  revealjs:\n    theme: simple\n    slide-number: true\n    scrollable: true\n    chalkboard: true\n    code-line-numbers: true\n    incremental: false\n---\n\n# Today's Agenda\n\n## What We'll Cover\n\n**Part 0: Questions from Last Week**\n- Github\n- R? dplyr? \n\n**Part 1: Algorithmic Decision Making** \n\n- What are algorithms in public policy?\n- When algorithmic decision making goes wrong\n- Current policy responses\n\n**Part 2: Active Learning**  \n\n- Small group scenarios: designing ethical algorithms\n- Discussion and reflection\n\n**Part 3: Census Data Foundations** \n\n- Understanding census data for policy analysis\n- Geography and data availability\n\n**Part 4: Hands-On Census Data with R** \n\n- Live demonstration of key functions\n- Practice exercises\n\n---\n\n# Part 1: Algorithmic Decision Making\n\n## Opening Question\n\n**Discuss with your table (1 minutes):**\n\nWhat is an algorithm?\n\n*Think beyond just computer code - how do you make decisions in daily life?*\n\n## What Is An Algorithm?\n\n**Definition:** A set of rules or instructions for solving a problem or completing a task\n\n**Examples:**\n\n- Recipe for cooking\n- Directions to get somewhere  \n- Decision tree for hiring\n\n- **Computer program that processes data to make predictions**\n\n## Algorithmic Decision Making in Government\n\n**Systems used to assist or replace human decision-makers**\n\nBased on predictions from models that process historical data containing:\n\n- **Inputs** (\"features\", \"predictors\", \"independent variables\", \"x\")\n- **Outputs** (\"labels\", \"outcome\", \"dependent variable\", \"y\")\n\n## Real-World Examples\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n**Criminal Justice**\nRecidivism risk scores for bail and sentencing decisions\n:::\n\n::: {.column width=\"30%\"}\n\n**Housing & Finance**  \nMortgage lending and tenant screening algorithms\n:::\n\n::: {.column width=\"30%\"}\n\n**Healthcare**\nPatient care prioritization and resource allocation\n:::\n\n::::\n\n## Clarifying Key Terms\n\n**Data Science** â†’ Computer science/engineering focus on algorithms and methods\n\n**Data Analytics** â†’ Application of data science methods to other disciplines  \n\n**Machine Learning** â†’ Algorithms for classification & prediction that learn from data\n\n**AI** â†’ Algorithms that adjust and improve across iterations (neural networks, etc.)\n\n## Public Sector Context\n\n**Long history of government data collection:**\n\n- Civic registration systems  \n- Census data\n- Administrative records\n- Operations research (post-WWII)\n\n**What's new?**\n\n- More data (official and \"accidental\")\n- Focus on **prediction** rather than explanation\n- **Harder to interpret** and explain\n\n## Why Government Uses Algorithms\n\n**Governments have limited budgets and need to serve everyone**\n\nAlgorithmic decision making is especially appealing because it promises:\n\n- **Efficiency** - process more cases faster\n- **Consistency** - same rules applied to everyone  \n- **Objectivity** - removes human bias\n- **Cost savings** - fewer staff needed\n\n**But does it deliver on these promises?**\n\n---\n\n# When Algorithms Go Wrong\n\n## Remember: Data Analytics Is Subjective\n\n**Every step involves human choices:**\n\n- Data cleaning decisions\n- Data coding or classification  \n- Data collection - use of imperfect proxies\n- How you interpret results\n- What variables you put in the model\n\n**These choices embed human values and biases**\n\n## {.smaller}\n**Healthcare Algorithm Bias**\n\n**The Problem:**\n\nAlgorithm used to identify high-risk patients for additional care **systematically discriminated against Black patients**\n\n**What Went Wrong:**\n\n- Algorithm used **healthcare costs as a proxy for need**\n- Black patients typically incur lower costs due to **systemic inequities in access**  \n- Result: Black patients under-prioritized despite **equivalent levels of illness**\n\n**Scale:** Used by hospitals and insurers for **over 200 million people** annually\n\n*Source: Obermeyer et al. (2019), Science*\n\n## Criminal Justice Algorithm Bias  \n\n**COMPAS Recidivism Prediction:**\n\n**The Problem:**\n\n- Algorithm **2x as likely** to falsely flag Black defendants as high risk\n- White defendants often rated low risk **even when they do reoffend**  \n\n**Why This Happens:**\n\n- Historical arrest data reflects **biased policing patterns**\n- Socioeconomic proxies correlate with race\n\n- **\"Objective\" data contains subjective human decisions**\n\n*Source: ProPublica investigation*\n\n## Dutch Welfare Fraud Detection\n\n**The Problem:**\n\n- **\"Black box\" system** operated in secrecy\n- Impossible for individuals to understand or challenge decisions\n- **Disproportionately targeted** vulnerable populations\n\n**Court Ruling:**\n\n- Breached privacy rights under European Convention on Human Rights\n- Highlighted **unfair profiling and discrimination**\n- System eventually **shut down**\n\n\n# Part 2: Active Learning Exercise\n\n## Small Group Challenge (10 Minutes! We keep a tight ship around here.)\n\nAt your table, pick **one** scenario and answer **three** prompts.\n\n**Prompts (plain English, no tech):**\n\n1. **Proxy:** What would you *use* to stand in for what you *want*?\n2. **Blind spot:** What data gap or historical bias could skew results?\n3. **Harm + Guardrail:** Who could be harmed, and one simple safeguard?\n\n\n## Pick one scenario\n\n1) **Emergency response prioritization** (natural disasters)  \n2) **School enrollment assignment**  \n3) **Automated traffic enforcement** (red-light cameras)  \n4) **Housing assistance allocation**  \n5) **Predictive policing**\n\n## Example (so you see the level)\n\n**Scenario:** Emergency response  \n\n- **Proxy:** 911 call volume â†’ stand-in for â€œneedâ€  \n- **Blind spot:** Under-calling where trust/connectivity is low  \n- **Harm + Guardrail:** Wealthier areas over-prioritized â†’ add a **vulnerability boost** (age/disability) and a **minimum-service floor** per zone\n\n## Discuss at your table (8 minutes)\n\n**Answer these out loud and on one device or notepad:**\n\n- **Proxy â†’** â€œWeâ€™d use ____ as a stand-in for ____.â€\n- **Blind spot â†’** â€œThis could miss/undercount ____ because ____.â€\n- **Harm + Guardrail â†’** â€œGroup ____ could be hurt by ____. Weâ€™d add ____ (one safeguard).â€\n\n**Choose ONE guardrail type:**\n\n- Prioritize vulnerable groups  \n- Cap disparities across areas (simple rule)  \n- Human review + appeals for edge cases  \n- Replace a bad proxy (collect the right thing)  \n- Publish criteria & run a periodic bias check\n\n## Lightning shares (2â€“3 tables)\n\nIn **â‰¤20 seconds**, say:\n\n- **Scenario**, **one proxy**, **one harm**, **one guardrail**\n\nClass quick poll: *Would that guardrail help?* \n\n- ðŸ‘ Green light \n- ðŸ‘Ž Red light\n\n\n---\n\n# Part 3: Census Data Foundations\n\n## Why Census Data Matters\n\n**Census data is the foundation for:**\n\n- Understanding community demographics\n- Allocating government resources  \n- Tracking neighborhood change\n\n- **Designing fair algorithms** (like those we just discussed)\n\n**Connection:** The same demographic data used in census goes into many of the algorithms we analyzed\n\n## Census vs. American Community Survey\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Decennial Census (2020)**\n\n- **Everyone** counted every 10 years\n- **9 basic questions:** age, race, sex, housing\n- **Constitutional requirement**\n- Determines political representation\n:::\n\n::: {.column width=\"50%\"}  \n**American Community Survey (ACS)**\n\n- **3% of households** surveyed annually\n- **Detailed questions:** income, education, employment, housing costs\n- **Replaced** the old \"long form\" in 2005\n- **A big source of data we'll use** this semester\n:::\n\n::::\n\n## ACS Estimates: What You Need to Know\n\n**1-Year Estimates** (areas > 65,000 people)\n\n- Most current data, smallest sample\n\n**5-Year Estimates** (all areas including census tracts)  \n\n- Most reliable data, largest sample\n- **What you'll use most often**\n\n**Key Point:** All ACS data comes with **margins of error** - we'll learn to work with uncertainty\n\n## Census Geography Hierarchy\n\n```\nNation\nâ”œâ”€â”€ Regions  \nâ”œâ”€â”€ States\nâ”‚   â”œâ”€â”€ Counties\nâ”‚   â”‚   â”œâ”€â”€ Census Tracts (1,500-8,000 people)\nâ”‚   â”‚   â”‚   â”œâ”€â”€ Block Groups (600-3,000 people)  \nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ Blocks (â‰ˆ85 people, Decennial only)\n```\n\n**Most policy analysis happens at:**\n\n- **County level** - state and regional planning\n- **Census tract level** - neighborhood analysis \n- **Block group level** - very local analysis (tempting, but big MOEs)\n\n## 2020 Census Innovation: Differential Privacy\n\n**The Challenge:** Modern computing can \"re-identify\" individuals from census data\n\n**The Solution:** Add mathematical \"noise\" to protect privacy while preserving overall patterns\n\n**The Controversy:** Some places now show populations living \"underwater\" or other impossible results\n\n**Why This Matters:** Even \"objective\" data involves **subjective choices** about privacy vs. accuracy. Also, the errors.\n\n## Accessing Census Data in R\n\n**Traditional approach:** Download CSV files from Census website\n\n**Modern approach:** Use R packages to access data directly\n\n**Benefits of programmatic access:**\n\n- Always get latest data\n- Reproducible workflows  \n- Automatic geographic boundaries\n- Built-in error handling\n\n**We'll use the `tidycensus` package starting in Lab 1**\n\n## Understanding ACS Data Structure\n\n**Data organized in tables:**\n\n- **B19013** - Median Household Income\n- **B25003** - Housing Tenure (Own/Rent)  \n- **B15003** - Educational Attainment\n- **B08301** - Commuting to Work\n\n**Each table has multiple variables:**\n\n- B19013_001E = Median household income (estimate)\n- B19013_001M = Median household income (margin of error)\n\n**You'll learn to find the right variables for your research questions**\n\n## Working with Margins of Error\n\n**Every ACS estimate comes with uncertainty**\n\n**Rule of thumb:**\n\n- Large MOE relative to estimate = **less reliable**\n- Small MOE relative to estimate = **more reliable**\n\n**In your analysis:**\n\n- Always report MOE alongside estimates\n- Be cautious comparing estimates with overlapping error margins\n- Consider using 5-year estimates for greater reliability\n\n## Two Types of Census Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Summary Tables** (what we'll use mostly)\n\n- Pre-calculated statistics by geography\n- Median income, percent college-educated, etc.\n- **Good for:** Mapping, comparing places\n:::\n\n::: {.column width=\"50%\"}  \n**PUMS - Individual Records** \n\n- Anonymous individual/household responses\n- **Good for:** Custom analysis, regression models\n- More complex but more flexible\n:::\n\n::::\n\n## When New Data Comes Out\n\n**ACS 1-year estimates:** Released in September (previous year's data)\n\n**ACS 5-year estimates:** Released in December  \n\n**Decennial Census:** Released on rolling schedule over 2-3 years\n\n**For Lab 1:** We'll use 2018-2022 ACS 5-year estimates (latest available)\n\n## Data Sources You'll Use\n\n**TIGER/Line Files** \n\n- Geographic boundaries (shapefiles)\n- Census tracts, counties, states\n- Now released as shapefiles (easier to use!)\n\n**Historical Data Sources:**\n\n- NHGIS (nhgis.org) - Historical census data\n- Neighborhood Change Database \n- Longitudinal Tract Database - Track changes over time\n\n---\n\n# Part 4: Hands-On Census Data with R\n\n## Live Demo Setup\n\nLet's see tidycensus in action with some basic examples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidycensus)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.6\nâœ” forcats   1.0.1     âœ” stringr   1.6.0\nâœ” ggplot2   4.0.1     âœ” tibble    3.3.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.2\nâœ” purrr     1.2.1     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(knitr)\n# Set API key (you'll get yours for Lab 1)\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n```\n\n\n:::\n:::\n\n\n**Follow along:** We'll work through examples together, then you'll practice in Lab 1\n\n## Basic get_acs() Function\n\n**Most important function you'll use:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get state-level population data\nstate_pop <- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",  # Total population\n  year = 2022,\n  survey = \"acs5\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nGetting data from the 2018-2022 5-year ACS\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(state_pop)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 52\nColumns: 5\n$ GEOID    <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"â€¦\n$ NAME     <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Coâ€¦\n$ variable <chr> \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B010â€¦\n$ estimate <dbl> 5028092, 734821, 7172282, 3018669, 39356104, 5770790, 3611317â€¦\n$ moe      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n```\n\n\n:::\n:::\n\n\n**Key parameters:** geography, variables, year, survey\n\n## Understanding the Output\n\n**Every ACS result includes:**\n\n- `GEOID` - Geographic identifier\n- `NAME` - Human-readable location name  \n- `variable` - Census variable code\n- `estimate` - The actual value\n- `moe` - Margin of error\n\n**This is the foundation for all your analysis**\n\n## Working with Multiple Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get income and population for Pennsylvania counties\npa_data <- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"  # Makes analysis easier\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nGetting data from the 2018-2022 5-year ACS\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(pa_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 6\n  GEOID NAME                 total_popE total_popM median_incomeE median_incomeM\n  <chr> <chr>                     <dbl>      <dbl>          <dbl>          <dbl>\n1 42001 Adams County, Pennsâ€¦     104604         NA          78975           3334\n2 42003 Allegheny County, Pâ€¦    1245310         NA          72537            869\n3 42005 Armstrong County, Pâ€¦      65538         NA          61011           2202\n4 42007 Beaver County, Pennâ€¦     167629         NA          67194           1531\n5 42009 Bedford County, Penâ€¦      47613         NA          58337           2606\n6 42011 Berks County, Pennsâ€¦     428483         NA          74617           1191\n```\n\n\n:::\n:::\n\n\n**Note:** `output = \"wide\"` gives you one row per place, multiple columns for variables\n\n## Data Cleaning Essentials\n\n**Clean up messy geographic names:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_clean <- pa_data %>%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    # Remove \"County\" word\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Compare before and after\nselect(pa_clean, NAME, county_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 Ã— 2\n   NAME                           county_name\n   <chr>                          <chr>      \n 1 Adams County, Pennsylvania     Adams      \n 2 Allegheny County, Pennsylvania Allegheny  \n 3 Armstrong County, Pennsylvania Armstrong  \n 4 Beaver County, Pennsylvania    Beaver     \n 5 Bedford County, Pennsylvania   Bedford    \n 6 Berks County, Pennsylvania     Berks      \n 7 Blair County, Pennsylvania     Blair      \n 8 Bradford County, Pennsylvania  Bradford   \n 9 Bucks County, Pennsylvania     Bucks      \n10 Butler County, Pennsylvania    Butler     \n# â„¹ 57 more rows\n```\n\n\n:::\n:::\n\n\n**Functions you'll use:** `str_remove()`, `str_extract()`, `str_replace()`\n\n## Calculating Data Reliability\n\n**This is crucial for policy work:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_reliability <- pa_clean %>%\n  mutate(\n    # Calculate MOE as percentage of estimate\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 2),\n    \n    # Create reliability categories\n    reliability = case_when(\n      moe_percentage < 5 ~ \"High Confidence\",\n      moe_percentage >= 5 & moe_percentage <= 10 ~ \"Moderate\",\n      moe_percentage > 10 ~ \"Low Confidence\"\n    )\n  )\n\ncount(pa_reliability, reliability)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n  reliability         n\n  <chr>           <int>\n1 High Confidence    57\n2 Moderate           10\n```\n\n\n:::\n:::\n\n\n**Key functions:** `case_when()` for categories, MOE calculations\n\n## Finding Patterns with dplyr\n\n- Find counties with highest uncertainty\n- Summarize by reliability category\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find counties with highest uncertainty\nhigh_uncertainty <- pa_reliability %>%\n  filter(moe_percentage > 8) %>%\n  arrange(desc(moe_percentage)) %>%\n  select(county_name, median_incomeE, moe_percentage)\n\n# Summary by reliability category  \nreliability_summary <- pa_reliability %>%\n  group_by(reliability) %>%\n  summarize(\n    counties = n(),\n    avg_income = round(mean(median_incomeE, na.rm = TRUE), 0)\n  )\n```\n:::\n\n\n## Professional Tables\n\n**Making results presentation-ready:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create formatted table\nkable(high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"MOE %\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Counties with Highest Income Data Uncertainty\n\n|County   | Median Income| MOE %|\n|:--------|-------------:|-----:|\n|Forest   |        46,188|  9.99|\n|Sullivan |        62,910|  9.25|\n\n\n:::\n:::\n\n\n**Key points:**\n\n- Use `kable()` for professional formatting\n- Add descriptive column names and captions  \n- Format numbers appropriately\n\n## Quick Practice\n\n**Try this with a neighbor (5 minutes):**\n\nUsing the `pa_reliability` data we just created:\n\n1. **Filter** for counties with \"High Confidence\" data\n2. **Arrange** by median income (highest first)  \n3. **Select** county name and median income\n4. **Slice** the top 3 counties\n\n**We'll share answers before moving on**\n\n## Policy Connection\n\n**Why this matters:**\n\n- **Algorithmic fairness:** Unreliable data can bias automated decisions\n- **Resource allocation:** Know which areas need extra attention\n- **Equity analysis:** Some communities may be systematically under-counted\n- **Professional credibility:** Always assess your data quality\n\n**This connects directly to our algorithmic bias discussion**\n\n---\n\n# Connecting the Dots\n\n## From Algorithms to Analysis\n\n**Today's key connections:**\n\n**Algorithmic Decision Making** â†’ Understanding why your analysis matters for real policy decisions\n\n**Data Subjectivity** â†’ Why we emphasize **transparent, reproducible methods** in this class\n\n**Census Data** â†’ The foundation for most urban planning and policy analysis\n\n**R Skills** â†’ The tools to do this work professionally and ethically\n\n## Questions for Reflection\n\n**As you work with data this semester, ask:**\n\n1. **What assumptions am I making** in my data choices?\n2. **Who might be excluded** from my analysis?\n3. **How could my findings be misused** if taken out of context?\n4. **What would I want policymakers to understand** about my methods?\n\nThese questions will make you a **more thoughtful analyst** and **better future policymaker**\n\n---\n\n# Next Steps\n\n## Before Next Class\n\n **Start Lab 1** - census data exploration (begins today!)\n **Aim to finish section 2** - we will continue this work in class next week\n\n## Questions?\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}