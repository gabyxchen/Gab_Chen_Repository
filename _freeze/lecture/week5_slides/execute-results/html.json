{
  "hash": "4d82f3cfcd79d046c76a3ab73efa6be0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Linear Regression\"\nsubtitle: \"Week 5: CPLN 5920-MUSA 5080\"\nauthor: \"Dr. Allison Lassiter\"\ndate: \"February 16, 2026\"\nformat: \n  revealjs:\n    theme: simple\n    slide-number: true\n    chalkboard: true\n    code-line-numbers: true\n    incremental: false\n    smaller: true\n---\n\n\n\n# Opening Question\n\n**Scenario:** You're advising a state agency on resource allocation.\n\nSome counties have sparse data. Can you **predict** their median income using data from counties with better measurements?\n\n**Broader question:** How do we make informed predictions when we don't have complete information?\n\n---\n\n# Today's Roadmap\n\n1. **The Statistical Learning Framework:** What are we actually doing?\n2. **Two goals:** Understanding relationships vs Making predictions\n3. **Building your first model** with PA census data\n4. **Model evaluation:** How do we know if it's any good?\n5. **Checking assumptions:** When can we trust the model?\n6. **Improving predictions:** Transformations, multiple variables\n\n---\n\n# Part 1: The Statistical Learning Framework\n\n## The General Problem\n\nWe observe data: counties, income, population, education, etc.\n\nWe believe there's some **relationship** between these variables.\n\n**Statistical learning** = a set of approaches for estimating that relationship\n\n\n::: {.cell}\n\n:::\n\n\n![](images/statistical_learning_framework.png){width=\"80%\"}\n\n---\n\n## Formalizing the Relationship\n\nFor any quantitative response Y and predictors X‚ÇÅ, X‚ÇÇ, ... X‚Çö:\n\n$$Y = f(X) + \\epsilon$$\n\nWhere:\n\n- **f** = the systematic information X provides about Y\n- **Œµ** = random error (irreducible)\n\n---\n\n## What is f?\n\n**f represents the true relationship** between predictors and outcome\n\n- It's **fixed** but **unknown**\n- It's what we're trying to estimate\n- Different X values produce different Y values through f\n\n**Example:**\n\n- Y = median income\n- X = population, education, poverty rate\n- f = the way these factors systematically relate to income\n\n---\n\n## Why Estimate f?\n\nTwo main reasons:\n\n**1. Prediction**\n\n- Estimate Y for new observations\n- Don't necessarily care about the exact form of f\n- Focus: accuracy of predictions\n\n**2. Inference**\n\n- Understand how X affects Y\n- Which predictors matter?\n- What's the nature of the relationship?\n- Focus: interpreting the model\n\n---\n\n## How Do We Estimate f?\n\n**Two broad approaches:**\n\n**Parametric Methods**\n\n- Make an assumption about the functional form (e.g., linear)\n- Reduces problem to estimating a few parameters\n- Easier to interpret\n- **This is what we'll focus on**\n\n**Non-Parametric Methods**\n\n- Don't assume a specific form\n- More flexible\n- Require more data\n- Harder to interpret\n\n---\n\n## Parametric vs. Non-Parametric\n\n![](images/parametric_vs_nonparametric.png)\n\n**Key difference:**\n\n- **Parametric (blue):** We assume f is linear, then estimate Œ≤‚ÇÄ and Œ≤‚ÇÅ\n- **Non-parametric (green):** We let the data determine the shape of f\n\n::: {.callout-note}\n## What about deep learning?\n\nNeural networks are technically parametric (millions of parameters!), but achieve flexibility through parameter quantity rather than assuming a rigid form. We won't cover them in this course, but they follow the same Y = f(X) + Œµ framework.\n:::\n\n---\n\n## Parametric Approach: Linear Regression\n\n**The assumption:** Relationship between X and Y is linear\n\n$$Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n\n**The task:** Estimate the Œ≤ coefficients using our sample data\n\n**The method:** Ordinary Least Squares (OLS)\n\n---\n\n## Why Linear Regression?\n\n**Advantages:**\n\n- Simple and interpretable\n- Well-understood properties\n- Works remarkably well for many problems\n- Foundation for more complex methods\n\n**Limitations:**\n\n- Assumes linearity (we'll test this)\n- Sensitive to outliers\n- Makes several assumptions (we'll check these)\n\n---\n\n# Part 2: Two Different Goals\n\n## Prediction vs Inference\n\nThe **same model** serves different purposes:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Inference**\n\n- \"Does education affect income?\"\n- Focus on coefficients\n- Statistical significance matters\n- Understand mechanisms\n:::\n\n::: {.column width=\"50%\"}\n**Prediction**\n\n- \"What's County Y's income?\"\n- Focus on accuracy\n- Prediction intervals matter\n- Don't need to understand why\n:::\n:::\n\n**Today:** We'll do both, but emphasize prediction\n\n---\n\n## Example: Prediction\n\n**Government use case:**\n\nCensus misses people in hard-to-count areas. Can we predict:\n\n- Income for areas with poor survey response?\n- Population for planning purposes?\n- Resource needs based on demographics?\n\n**The model doesn't explain WHY** these relationships exist, but if predictions are accurate, they\ncan be useful for policy\n\n---\n\n## Example: Inference\n\n**Research use case:**\n\nUnderstanding gentrification:\n\n- Which neighborhood characteristics explain income change?\n- How much does education matter vs. proximity to downtown?\n- Are policy interventions associated with outcomes?\n\n**Here we care about the coefficients** and what they tell us about mechanisms\n\n---\n\n## Connection to Week 2: Algorithmic Bias\n\nRemember the healthcare algorithm that discriminated?\n\n**The model:** Predicted healthcare needs using costs as proxy\n\n**Technically:** Probably had good R¬≤, low prediction error (good \"fit\")\n\n**Ethically:** Learned and amplified existing discrimination\n\n::: {.callout-important}\n## Critical Point\n\nA model can be statistically \"good\" while being ethically terrible for decision-making.\n:::\n\n---\n\n# Part 3: Building Your First Model\n\n## Start with Data and Visualization\n\nLet's apply these concepts to PA counties:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week5_slides_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n---\n\n## What Do We See?\n\n**Before fitting any model, discuss the visualization:**\n\n- Generally positive relationship\n- Considerable scatter (not deterministic)\n- Most counties are small (clustered left)\n- One large county with surprisingly low income\n- Wider confidence band at higher populations\n\n**Question:** What does this tell us about f(X)?\n\n---\n\n## Fit the Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = median_incomeE ~ total_popE, data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-39536  -6013  -1868   5075  44196 \n\nCoefficients:\n                Estimate   Std. Error t value             Pr(>|t|)    \n(Intercept) 62855.819808  1760.477709  35.704 < 0.0000000000000002 ***\ntotal_popE      0.021477     0.005192   4.137             0.000103 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11820 on 65 degrees of freedom\nMultiple R-squared:  0.2084,\tAdjusted R-squared:  0.1962 \nF-statistic: 17.11 on 1 and 65 DF,  p-value: 0.0001032\n```\n\n\n:::\n:::\n\n\n---\n\n## Interpreting Coefficients\n\n**Intercept (Œ≤‚ÇÄ) = $62,855**\n\n- Expected income when population = 0\n- Not usually meaningful in practice\n\n**Slope (Œ≤‚ÇÅ) = $0.02**\n\n- For each additional person, income increases by $0.02\n- **More useful:** For every 1,000 people, income increases by ~$20\n\n**Is this relationship real?**\n\n- Null hypothesis is Œ≤‚ÇÅ = 0 (population has no relationship to income)\n  (Note: note, not testing causality, like \"population has no effect on income\")\n- p-value < 0.001 ‚Üí Very unlikely to see this if true Œ≤‚ÇÅ = 0\n- We can reject the null hypothesis\n\n---\n\n## The \"Holy Grail\" Concept\n\n::: {.columns}\n::: {.column width=\"45%\"}\nOur estimates are just that: **estimates** of the true (unknown) parameters\n\n**Key insight:**\n\n- Red line = true relationship (unknowable)\n- Blue line = our estimate from this sample (n) of the total population (N)\n- Different samples (n) of the same population (N) ‚Üí slightly different blue lines\n- Standard errors quantify this uncertainty\n:::\n\n::: {.column width=\"55%\"}\n![](images/population_vs_sample_regression.png)\n:::\n:::\n\n---\n\n## Statistical Significance\n\n**The logic:**\n\n1. **Null hypothesis (H‚ÇÄ):** Œ≤‚ÇÅ = 0 (no relationship)\n2. **Our estimate:** Œ≤‚ÇÅ = 0.02\n3. **Question:** Could we get 0.02 just by chance if H‚ÇÄ is true?\n\n**t-statistic:** How many standard errors away from 0?\n\n- Bigger |t| = more confidence the relationship is real\n- Rule of thumb: |t| > 3 indicates strong evidence the relationship is real\n\n**p-value:** Probability of seeing our estimate if H‚ÇÄ is true\n\n- Small p ‚Üí reject H‚ÇÄ, conclude relationship exists\n\n---\n\n# Part 4: Model Evaluation\n\n## How Good is This Model?\n\n**Two key questions:**\n\n1. **How well does it fit the data we used?** (in-sample fit)\n2. **How well would it predict new data?** (out-of-sample performance)\n\n**These are NOT the same thing!**\n\n---\n\n## In-Sample Fit: R¬≤\n\n**R¬≤ = 0.208**\n\n\"21% of variation in income is explained by population\"\n\n**Is this good?**\n\n- Depends on your goal!\n- For prediction: Moderate\n- For inference: Shows population matters, but other factors exist\n\n**R¬≤ alone doesn't tell us if the model is trustworthy**\n\n---\n\n## The Problem: Overfitting\n\n**Three scenarios:**\n\n1. **Underfitting:** Model too simple (high bias)\n2. **Good fit:** Captures pattern without noise\n3. **Overfitting:** Memorizes training data (high variance)\n\n---\n\n## Overfitting in Regression\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week5_slides_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n**The danger:** High R¬≤ doesn't mean good predictions!\n\n---\n\n## Train/Test Split\n\n**Solution:** Hold out some data to test predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n:::\n\n\n---\n\n## Evaluate Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate prediction error (RMSE)\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining RMSE: 12893 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest RMSE: 9536 \n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Interpreting RMSE\n\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\n:::\n\n---\n\n## Cross-Validation\n\n**Better approach:** Multiple train/test splits\n\n![](images/cv-illustration.svg){width=\"80%\"}\n\n**Gives more stable estimate of true prediction performance**\n\n---\n\n## Cross-Validation in Action {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 12577.76 0.5643826 8859.865 5609.002  0.2997098 2238.042\n```\n\n\n:::\n:::\n\n\n**Key Metrics (Averaged Across 10 Folds)**\n\n- **RMSE:** Typical prediction error (~$12,578) - penalizes large errors more\n- **R¬≤:** % of variation explained (0.564)\n- **MAE:** Average absolute error (~$8,860) - easier to interpret\n\n---\n\n# Part 5: Checking Assumptions\n\n## When Can We Trust This Model?\n\nLinear regression makes assumptions. If violated:\n\n- Coefficients may be biased\n- Standard errors wrong\n- Predictions unreliable\n\n**We must check diagnostics** before trusting any model\n\n---\n\n## Assumption 1: Linearity\n\n**What we assume:** Relationship is actually linear\n\n**How to check:** Residual plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](week5_slides_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n---\n\n## Reading Residual Plots\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Good**\n\n- Random scatter\n- Points around 0\n- Constant spread\n:::\n\n::: {.column width=\"50%\"}\n**Bad**\n\n- Curved pattern\n- Model missing something\n- Predictions biased\n:::\n:::\n\n::: {.callout-important}\n## Why This Matters for Prediction\n\n**Linearity violations hurt predictions, not just inference:**\n\n- If the true relationship is curved and you fit a straight line, you'll systematically underpredict in some regions and overpredict in others\n- **Biased predictions** in predictable ways (not random errors!)\n- Residual plots should show **random scatter** - any pattern means your model is missing something systematic\n:::\n\n---\n\n## Assumption 2: Constant Variance\n\n**Heteroscedasticity:** Variance changes across X\n\n**Impact:** Standard errors are wrong ‚Üí p-values misleading\n\n::: {.callout-warning}\n## What Heteroskedasticity Tells You\n\n**Often a symptom of model misspecification:**\n\n- Model fits well for some values (e.g., small counties) but poorly for others (large counties)\n- May indicate **missing variables** that matter more at certain X values\n- Ask: \"What's different about observations with large residuals?\"\n\n**Example:** Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n:::\n\n---\n\n## Heteroskedasticity Visualized\n\n![](images/heteroskedasticity-visual.svg){width=\"90%\"}\n\n**Key Insight:** Adding the right predictor can fix heteroscedasticity\n\n---\n\n## Formal Test: Breusch-Pagan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\nbptest(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  model1\nBP = 27.481, df = 1, p-value = 0.0000001587\n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\n- **p > 0.05:** Constant variance assumption OK\n- **p < 0.05:** Evidence of heteroscedasticity\n\n**If detected, solutions:**\n\n1. Transform Y (try `log(income)`)\n2. Robust standard errors\n3. Add missing variables\n4. Accept it (point predictions still OK for some prediction goals) -- hetroscedasticity will affect predcition of the error, not the coefficients.\n\n---\n\n## Assumption: Normality of Residuals\n\n**What we assume:** Residuals are normally distributed\n\n**Why it matters:**\n\n- Less critical for **point predictions** (unbiased regardless)\n- Important for **confidence intervals** and **prediction intervals**\n- Needed for valid hypothesis tests (t-tests, F-tests)\n\n# Q-Q Plot\n\n::: {.cell}\n::: {.cell-output-display}\n![](week5_slides_files/figure-revealjs/unnamed-chunk-11-1.png){width=480}\n:::\n:::\n\n\n\n---\n\n## Assumption 3: No Multicollinearity\n**For multiple regression:** Predictors shouldn't be too correlated\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n:::\n\n\n**Why it matters:** Coefficients become unstable, hard to interpret\n\n---\n\n## Assumption 4: No Influential Outliers\n\n**Not all outliers are problems** - only those with high leverage AND large residuals\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Visual Diagnostic\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week5_slides_files/figure-revealjs/unnamed-chunk-13-1.png){width=480}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n### Identify Influential Points\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 4\n  NAME                              total_popE median_incomeE cooks_d\n  <chr>                                  <dbl>          <dbl>   <dbl>\n1 Philadelphia County, Pennsylvania    1593208          57537   5.95 \n2 Allegheny County, Pennsylvania       1245310          72537   0.399\n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\n- Cook's D > 4/n = potentially influential\n- High leverage + large residual = pulls regression line\n:::\n:::\n\n---\n\n## What To Do With Influential Points\n\n::: {.callout-tip}\n## Investigation Strategy\n\n1. **Investigate:** Why is this observation unusual? (data error? truly unique?)\n2. **Report:** Always note influential observations in your analysis\n3. **Sensitivity check:** Refit model without them - do conclusions change?\n4. **Don't automatically remove:** They might represent real, important cases\n\n**For policy:** An influential county might need **special attention**, not exclusion!\n:::\n\n::: {.callout-important}\n## Connection to Algorithmic Bias\n\nHigh-influence observations in demographic could represent **marginalized communities** or unique populations. Automatically removing them can erase important populations from analysis and lead to biased policy decisions.\n\n**Always investigate before removing!**\n:::\n\n---\n\n# Part 6: Improving the Model\n\n## Adding More Predictors\n\nMaybe population alone isn't enough:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = median_incomeE ~ total_popE + percent_collegeE + \n    poverty_rateE, data = pa_data_full)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32939  -4473  -1098   3861  27672 \n\nCoefficients:\n                    Estimate  Std. Error t value             Pr(>|t|)    \n(Intercept)      60893.86796  1589.24122  38.316 < 0.0000000000000002 ***\ntotal_popE           0.06657     0.03356   1.984               0.0517 .  \npercent_collegeE     0.04710     0.14916   0.316               0.7532    \npoverty_rateE       -0.36503     0.07749  -4.711             0.000014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8610 on 63 degrees of freedom\nMultiple R-squared:  0.5931,\tAdjusted R-squared:  0.5738 \nF-statistic: 30.61 on 3 and 63 DF,  p-value: 0.000000000002485\n```\n\n\n:::\n:::\n\n\n---\n\n## Log Transformations\n\nIf relationship is curved, try transforming:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare linear vs log\nmodel_linear <- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = median_incomeE ~ log(total_popE), data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-27231  -5780  -2118   3953  40638 \n\nCoefficients:\n                Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)        -4867      12374  -0.393      0.695    \nlog(total_popE)     6276       1074   5.842 0.00000018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10760 on 65 degrees of freedom\nMultiple R-squared:  0.3443,\tAdjusted R-squared:  0.3342 \nF-statistic: 34.13 on 1 and 65 DF,  p-value: 0.0000001805\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check which residual plot looks better\n```\n:::\n\n\n**Interpretation changes:** Log models show percentage relationships\n\n---\n\n## Categorical Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create metro/non-metro indicator\npa_data <- pa_data %>%\n  mutate(metro = ifelse(total_popE > 500000, 1, 0))\n\nmodel3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = median_incomeE ~ total_popE + metro, data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28825  -6732  -2885   7672  26622 \n\nCoefficients:\n                Estimate   Std. Error t value             Pr(>|t|)    \n(Intercept) 64924.857892  1659.821786  39.116 < 0.0000000000000002 ***\ntotal_popE     -0.005290     0.008047  -0.657             0.513257    \nmetro       29865.529225  7319.309190   4.080             0.000127 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10610 on 64 degrees of freedom\nMultiple R-squared:  0.3718,\tAdjusted R-squared:  0.3522 \nF-statistic: 18.94 on 2 and 64 DF,  p-value: 0.0000003456\n```\n\n\n:::\n:::\n\n\n**R creates dummy variables automatically**\n\n---\n\n## Summary: The Regression Workflow\n\n1. **Understand the framework:** What's f? What's the goal?\n2. **Visualize first:** Does a linear model make sense?\n3. **Fit the model:** Estimate coefficients\n4. **Evaluate performance:** Train/test split, cross-validation\n5. **Check assumptions:** Residual plots, VIF, outliers\n6. **Improve if needed:** Transformations, more variables\n7. **Consider ethics:** Who could be harmed by this model?\n\n---\n\n### Key Takeaways {.smaller style=\"line-height: 1.1;\"}\n\n**Statistical Learning:**\n\n- We're estimating f(X), the systematic relationship\n- Parametric methods assume a form (we chose linear)\n\n**Two purposes:**\n\n- Inference: understand relationships\n- Prediction: forecast new values\n\n**Model evaluation:**\n\n- In-sample fit ‚â† out-of-sample performance\n- Beware overfitting!\n\n**Diagnostics matter:**\n\n- Always check assumptions\n- Plots reveal what R¬≤ hides\n\n---\n\n\n# üèÜ Learning- Focused In-Class Challenge: \n*Best Predictive Model Competition*\n\n## The Task\n\n**Predict median home value** (`B25077_001`) for PA counties using any combination of predictors\n\nBuild the model with **lowest 10-fold cross-validated RMSE**\n\n## Available Predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchallenge_data <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    home_value = \"B25077_001\",      # YOUR TARGET\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\",      # Median age\n    percent_college = \"B15003_022\", # Bachelor's degree or higher\n    median_rent = \"B25058_001\",     # Median rent\n    poverty_rate = \"B17001_002\"     # Population in poverty\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n```\n:::\n\n\n## Rules & Strategy {.smaller style=\"line-height: 1.1;\"}\n\n**You can:**\n\n- Use any combination of predictors (time permitting, you can fetch more)\n- Try log transformations: `log(total_popE)`\n- Engineer new categorical features\n- Remove influential outliers (but document which!)\n\n**You must:**\n\n- Use 10-fold cross-validation to report final RMSE\n- Do a full diagnostic check (residual plot, Cook's D, or Breusch-Pagan)\n- Be ready to explain your model in 2 minutes\n\n::: {.callout-tip}\n## Hints\n- Start simple (one predictor), check diagnostics\n- Income and rent are probably highly correlated (multicollinearity!)\n- Try log transformation if relationship looks curved\n- Don't forget to remove NAs: `na.omit(challenge_data)`\n:::\n\n",
    "supporting": [
      "week5_slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}